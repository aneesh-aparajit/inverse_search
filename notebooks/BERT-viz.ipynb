{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12af6007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import yaml\n",
    "\n",
    "from dataclasses import dataclass, fields\n",
    "\n",
    "# read the config file\n",
    "with open(\"../src/config/config_v1.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BertOutputDataClass:\n",
    "    bert_output: torch.Tensor\n",
    "    bert_pooled_output: torch.Tensor\n",
    "    attention_weights: torch.Tensor\n",
    "\n",
    "\n",
    "class BertModel(nn.Module):\n",
    "    def __init__(self, config: Dict[str, any]) -> None:\n",
    "        super(BertModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=config[\"LANGUAGE\"][\"BASELINE_CONFIG\"][\"NUM_EMBEDDINGS\"],\n",
    "            embedding_dim=config[\"LANGUAGE\"][\"BASELINE_CONFIG\"][\"EMBEDDING_DIM\"],\n",
    "        )\n",
    "        self.position_embeddings = BertEmbeddings(\n",
    "            model_dimension=config[\"LANGUAGE\"][\"BASELINE_CONFIG\"][\"MODEL_DIMENSION\"]\n",
    "        )\n",
    "\n",
    "        self.encoder = BertEncoder(\n",
    "            model_dimension=config[\"LANGUAGE\"][\"BASELINE_CONFIG\"][\"MODEL_DIMENSION\"],\n",
    "            embedding_dim=config[\"LANGUAGE\"][\"BASELINE_CONFIG\"][\"EMBEDDING_DIM\"],\n",
    "            hidden_size=config[\"LANGUAGE\"][\"BASELINE_CONFIG\"][\"HIDDEN_SIZE\"],\n",
    "            num_encoder_layers=config[\"LANGUAGE\"][\"BASELINE_CONFIG\"][\n",
    "                \"NUM_ENCODER_LAYERS\"\n",
    "            ],\n",
    "            intermediate_size=config[\"LANGUAGE\"][\"BASELINE_CONFIG\"][\n",
    "                \"INTERMEDIATE_SIZE\"\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        self.pooler = BertPooler(\n",
    "            in_features=config[\"LANGUAGE\"][\"BASELINE_CONFIG\"][\"MODEL_DIMENSION\"],\n",
    "            out_features=config[\"LANGUAGE\"][\"BASELINE_CONFIG\"][\"MODEL_DIMENSION\"],\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> BertOutputDataClass:\n",
    "        x_embeddings = self.embedding(x)\n",
    "        x_embeddings = self.position_embeddings(x_embeddings)\n",
    "        weights, output = self.encoder.forward(x_embeddings)\n",
    "        bert_output, bert_pooled_output = self.pooler(output)\n",
    "        return BertOutputDataClass(\n",
    "            bert_output=bert_output,\n",
    "            bert_pooled_output=bert_pooled_output,\n",
    "            attention_weights=weights,\n",
    "        )\n",
    "\n",
    "\n",
    "class BertPooler(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int) -> None:\n",
    "        super(BertPooler, self).__init__()\n",
    "        self.dense = nn.Linear(\n",
    "            in_features=in_features, out_features=out_features, bias=True\n",
    "        )\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        bert_output = self.activation(self.dense(x))\n",
    "        bert_pooled_output = bert_output.mean(axis=1)\n",
    "        return bert_output, bert_pooled_output\n",
    "\n",
    "\n",
    "class BertSelfAttention(nn.Module):\n",
    "    \"\"\"Implemented the Scaled Dot Product Attention proposed in Attention is All You Need\n",
    "    by Vaswani et. al (page 4, section 3.2)\n",
    "\n",
    "    Args:\n",
    "    :param: `model_dimension`: int -> represents the embedding dimension which the transformer will handle\n",
    "    :param: `embedding_dim`: int -> represents the  dimension of the input vector, from the nn.Embedding layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, head_dim: int, embedding_dim: int) -> None:\n",
    "        super(BertSelfAttention, self).__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.query = nn.Linear(\n",
    "            in_features=embedding_dim, out_features=head_dim\n",
    "        )  # find the projection of input for query\n",
    "        self.key = nn.Linear(\n",
    "            in_features=embedding_dim, out_features=head_dim\n",
    "        )  # find the projection of input for key\n",
    "        self.value = nn.Linear(\n",
    "            in_features=embedding_dim, out_features=head_dim\n",
    "        )  # find the projection of input for value\n",
    "        self.dropout = nn.Dropout(p=config[\"LANGUAGE\"][\"BASELINE_CONFIG\"][\"DROPOUT\"])\n",
    "\n",
    "    @staticmethod\n",
    "    def scaled_dot_product_attention(\n",
    "        Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        weights = F.softmax(torch.bmm(Q, K.mT) / math.sqrt(Q.shape[-1]), dim=-1)\n",
    "        outputs = torch.bmm(weights, V)\n",
    "        return weights, outputs\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        Q, K, V = self.query(x), self.key(x), self.value(x)\n",
    "        weights, outputs = self.scaled_dot_product_attention(Q, K, V)\n",
    "        return weights, self.dropout(outputs)\n",
    "\n",
    "\n",
    "class BertAttention(nn.Module):\n",
    "    \"\"\"Implements the Multi-head attention as mentioned in the Attention is All you Need by Vaswani et. al.\n",
    "\n",
    "    Args:\n",
    "    :param: `model_dimension`: int -> represents the embedding dimension which the transformer will handle\n",
    "    :param: `embedding_dim`: int -> represents the  dimension of the input vector, from the nn.Embedding layer\n",
    "    :param: `num_heads`: int -> represents the number of heads needed for the training\n",
    "\n",
    "    How does this work?\n",
    "    -------------------\n",
    "        * So, what we want to do is, find projections of the input embedding vector to the queries, keys and the values.\n",
    "        * There are multiple implementations for the same, one which could be to reshape the tensors and implement self attention\n",
    "        on top of that, which is more efficient\n",
    "        implementation, but for more readability and understanding, what we can do is the following:\n",
    "            * We make the model learn the projects and map it to another dimension which is\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, model_dimension: int, embedding_dim: int, num_heads: int = 8\n",
    "    ) -> None:\n",
    "        super(BertAttention, self).__init__()\n",
    "        self.model_dimension = model_dimension\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        assert (\n",
    "            self.embedding_dim % self.num_heads == 0\n",
    "        ), \"Make sure that the, number of heads is divisible by the embedding size\"\n",
    "        self.head_dim = self.model_dimension // self.num_heads\n",
    "\n",
    "        self.heads = nn.ModuleList(\n",
    "            [\n",
    "                BertSelfAttention(\n",
    "                    embedding_dim=self.embedding_dim, head_dim=self.head_dim\n",
    "                )\n",
    "                for _ in range(self.num_heads)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.output = BertSelfOutput(\n",
    "            in_features=self.model_dimension, out_features=self.model_dimension\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        outputs = torch.tensor([])\n",
    "        weights = torch.tensor([])\n",
    "\n",
    "        for head in self.heads:\n",
    "            weight, output = head(x)\n",
    "            outputs = torch.cat([outputs, output], dim=-1)\n",
    "            weights = torch.cat([weights, weight], dim=-1)\n",
    "        \n",
    "        outputs = self.output(outputs)\n",
    "\n",
    "        return weights, outputs\n",
    "\n",
    "\n",
    "class BertSelfOutput(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int) -> None:\n",
    "        super(BertSelfOutput, self).__init__()\n",
    "        self.dense = nn.Linear(\n",
    "            in_features=in_features, out_features=out_features, bias=True\n",
    "        )\n",
    "        self.LayerNorm = nn.LayerNorm(\n",
    "            normalized_shape=(out_features,), eps=1e-12, elementwise_affine=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=config[\"LANGUAGE\"][\"BASELINE_CONFIG\"][\"DROPOUT\"])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.dropout(self.LayerNorm(self.dense(x)))\n",
    "\n",
    "\n",
    "class BertIntermediate(nn.Module):\n",
    "    def __init__(self, in_features: int, intermediate_size: int) -> None:\n",
    "        super(BertIntermediate, self).__init__()\n",
    "        self.dense = nn.Linear(in_features=in_features, out_features=intermediate_size)\n",
    "        self.intermediate_act_fn = nn.GELU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.intermediate_act_fn(self.dense(x))\n",
    "\n",
    "\n",
    "class BertOutput(nn.Module):\n",
    "    \"\"\"Implements the FFN proposed by Vaswani Et. al in Attention is All you Need (page 5, section 3.3)\n",
    "\n",
    "    Args:\n",
    "    :param: `model_dimension`: int -> represents the embedding dimension which the transformer will handle\n",
    "    :param: `embedding_dim`: int -> represents the  dimension of the input vector, from the nn.Embedding layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_dimension: int, hidden_size: int) -> None:\n",
    "        super(BertOutput, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_features=model_dimension, out_features=hidden_size)\n",
    "        self.linear2 = nn.Linear(in_features=hidden_size, out_features=model_dimension)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.gelu(self.linear1(x))\n",
    "        return self.linear2(x)\n",
    "\n",
    "\n",
    "class BertEmbeddings(nn.Module):\n",
    "    def __init__(self, model_dimension: int = 512, max_token_length: int = 5000):\n",
    "        super(BertEmbeddings, self).__init__()\n",
    "\n",
    "        self.position_encoding_matrix = torch.zeros((max_token_length, model_dimension))\n",
    "        position_ids = torch.arange(max_token_length).unsqueeze(1)\n",
    "        dimension_ids = torch.arange(model_dimension)\n",
    "\n",
    "        self.position_encoding_matrix = torch.where(\n",
    "            dimension_ids % 2 == 0,\n",
    "            torch.sin(position_ids / 10000 ** (2 * dimension_ids / model_dimension)),\n",
    "            torch.cos(position_ids / 10000 ** (2 * dimension_ids / model_dimension)),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=config[\"LANGUAGE\"][\"BASELINE_CONFIG\"][\"DROPOUT\"])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.dropout(\n",
    "            x + self.position_encoding_matrix[: x.shape[1]]\n",
    "        )  # Take sequence length\n",
    "\n",
    "\n",
    "class BertLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self, model_dimension: int, embedding_dim: int, hidden_size: int\n",
    "    ) -> None:\n",
    "        super(BertLayer, self).__init__()\n",
    "        self.model_dimension = model_dimension\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(normalized_shape=self.model_dimension)\n",
    "        self.layernorm2 = nn.LayerNorm(normalized_shape=self.model_dimension)\n",
    "\n",
    "        self.mha = BertAttention(\n",
    "            model_dimension=self.model_dimension, embedding_dim=self.embedding_dim\n",
    "        )\n",
    "        self.output = BertOutput(\n",
    "            model_dimension=self.model_dimension, hidden_size=self.hidden_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # mha\n",
    "        x_ = x.clone()\n",
    "        weights, x = self.mha(x)\n",
    "        x = x_ + self.layernorm1(x)\n",
    "        # ffn\n",
    "        x_ = x.clone()\n",
    "        x = self.output(x)\n",
    "        x = x_ + self.layernorm2(x)\n",
    "\n",
    "        return weights, x\n",
    "\n",
    "\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_dimension: int,\n",
    "        embedding_dim: int,\n",
    "        intermediate_size: int,\n",
    "        hidden_size: int,\n",
    "        num_encoder_layers: int,\n",
    "    ) -> None:\n",
    "        super(BertEncoder, self).__init__()\n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        self.model_dimension = model_dimension\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_dim\n",
    "        self.intermediate_size = intermediate_size\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                BertLayer(\n",
    "                    model_dimension=self.model_dimension,\n",
    "                    embedding_dim=self.embedding_size,\n",
    "                    hidden_size=self.hidden_size,\n",
    "                )\n",
    "                for _ in range(self.num_encoder_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.intermediate = BertIntermediate(\n",
    "            in_features=self.model_dimension,\n",
    "            intermediate_size=self.intermediate_size,\n",
    "        )\n",
    "\n",
    "        self.output = BertSelfOutput(\n",
    "            in_features=self.intermediate_size, out_features=self.hidden_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[Dict[str, torch.Tensor], torch.Tensor]:\n",
    "        weights = {}\n",
    "        for layer_num, layer in enumerate(self.layers):\n",
    "            wei, x = layer(x)\n",
    "            weights[f\"layer_{layer_num}\"] = wei\n",
    "        x = self.intermediate(x)\n",
    "        # print(x.shape, self.output.parameters)\n",
    "        x = self.output(x)\n",
    "        return weights, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a95ae46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchview import draw_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbdd3bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randint(\n",
    "    low=0,\n",
    "    high=config[\"LANGUAGE\"][\"BASELINE_CONFIG\"][\"NUM_EMBEDDINGS\"],\n",
    "    size=(32, 128),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df3e8800",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_graph = draw_graph(\n",
    "    BertModel(config=config), input_data=inputs,\n",
    "    graph_name='BertModel', \n",
    "    expand_nested=True, save_graph=True, filename=\"BertModel\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
